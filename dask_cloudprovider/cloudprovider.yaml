cloudprovider:
  ecs:
    fargate_scheduler: False # Use fargate mode for the scheduler
    fargate_workers: False # Use fargate mode for the workers
    scheduler_cpu: 1024 # Millicpu (1024ths of a CPU core)
    scheduler_mem: 4096 # Memory in MB
    #   scheduler_extra_args: "--tls-cert,/path/to/cert.pem,--tls-key,/path/to/cert.key,--tls-ca-file,/path/to/ca.key"
    worker_cpu: 4096 # Millicpu (1024ths of a CPU core)
    worker_mem: 16384 # Memory in MB
    worker_gpu: 0 # Number of GPUs for each worker
    #   worker_extra_args: "--tls-cert,/path/to/cert.pem,--tls-key,/path/to/cert.key,--tls-ca-file,/path/to/ca.key"
    n_workers: 0 # Number of workers to start the cluster with
    scheduler_timeout: "5 minutes" # Length of inactivity to wait before closing the cluster

    image: "daskdev/dask:latest" # Docker image to use for non GPU tasks
    gpu_image: "rapidsai/rapidsai:latest" # Docker image to use for GPU tasks
    cluster_name_template: "dask-{uuid}" # Template to use when creating a cluster
    cluster_arn: "" # ARN of existing ECS cluster to use (if not set one will be created)
    execution_role_arn: "" # Arn of existing execution role to use (if not set one will be created)
    task_role_arn: "" # Arn of existing task role to use (if not set one will be created)
    task_role_policies: [] # List of policy arns to attach to tasks (e.g S3 read only access)
    #   platform_version: "LATEST" # Fargate platformVersion string like "1.4.0" or "LATEST"

    cloudwatch_logs_group: "" # Name of existing cloudwatch logs group to use (if not set one will be created)
    cloudwatch_logs_stream_prefix: "{cluster_name}" # Stream prefix template
    cloudwatch_logs_default_retention: 30 # Number of days to retain logs (only applied if not using existing group)

    vpc: "default" # VPC to use for tasks
    subnets: [] # VPC subnets to use (will use all available if not set)
    security_groups: [] # Security groups to use (if not set one will be created)

    tags: {} # Tags to apply to all AWS resources created by the cluster manager
    environment: {} # Environment variables that are set within a task container
    find_address_timeout: 60 # Configurable timeout in seconds for finding the task IP from the cloudwatch logs.
    skip_cleanup: False # Skip cleaning up of stale resources

  ec2:
    region: null # AWS region to create cluster. Defaults to environment or account default region.
    bootstrap: true # It is assumed that the AMI does not have Docker and needs bootstrapping. Set this to false if using a custom AMI with Docker already installed.
    auto_shutdown: true # Shutdown instances automatically if the scheduler or worker services time out.
    # worker_command: "dask-worker" # The command for workers to run. If the instance_type is a GPU instance dask-cuda-worker will be used.
    # ami: "" # AMI ID to use for all instances. Defaults to latest Ubuntu 20.04 image.
    instance_type: "t2.micro" # Instance type for all workers
    # vpc: "" # VPC id for instances to join. Defaults to default VPC.
    # subnet_id: "" # Subnet ID for instances to. Defaults to all subnets in default VPC.
    # security_groups: [] # Security groups for instances. Will create a minimal Dask security group by default.
    filesystem_size: 40 # Default root filesystem size for scheduler and worker VMs in GB

  azure:
    experiment_name: "dask-cloudprovider" # default name of the Experiment to submit
    initial_node_count: 1 # Initial node count
    jupyter: False # by default start the Jupyter lab instance on the headnode
    jupyter_port: 9000 # default Jupyter lab port on the headnode
    dashboard_port: 9001 # default Dask dashboard port on the headnode
    scheduler_port: 9002 # default for the scheduler port if using from outside of cluster VNET
    scheduler_idle_timeout: 1200 # default timeout for dask-scheduler process to shutdown if idle (in seconds)
    worker_death_timeout: 30 # default timeout for dask-worker process upon losing scheduler
    additional_ports: [] # list of tuples of additional ports to map/forward
    admin_username: "" # username to log in to the AzureML Training Cluster for 'local' runs
    admin_ssh_key: "" # password to log in to the AzureML Training Cluster for 'local' runs
    telemetry_opt_out: False # by default we log the version of the AzureMLCluster being used, set to True to opt-out
    datastores: [] # default list of datastores to mount

  digitalocean:
    token: null # API token for interacting with the Digital Ocean API
    region: "nyc3" # Region to launch Droplets in
    size: "s-1vcpu-1gb" # Droplet size to launch, default is 1GB RAM, 1 vCPU
    image: "ubuntu-20-04-x64" # Operating System image to use
